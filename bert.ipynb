{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JLiQufAET4j"
      },
      "outputs": [],
      "source": [
        "!pip install datasets scikit-multilearn transformers\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
        "\n",
        "target_labels = [\"happiness\", \"surprise\", \"disgust\", \"anger\", \"neutral\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhJV5uBFLP-X"
      },
      "source": [
        "#Reformat and Save Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwp6MZswHv6E"
      },
      "source": [
        "##Process and save emotion data as csv for manual labelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjXMBoHFEFHp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "target_labels = [\"happiness\", \"surprise\", \"disgust\", \"anger\", \"neutral\"]\n",
        "\n",
        "def map_data_to_column(example):\n",
        "  if example['label'] is 1:\n",
        "    example['happiness'] = 1\n",
        "  if example['label'] is 3:\n",
        "    example['anger'] = 1\n",
        "  if example['label'] is 5:\n",
        "    example['surprise'] = 1\n",
        "  return example\n",
        "\n",
        "datasets_emotion = load_dataset(\"emotion\")\n",
        "emotion_dataset = datasets_emotion.get(\"train\")\n",
        "emotion_dataset = emotion_dataset.select(range(500))\n",
        "dummy_values = [0] * len(emotion_dataset)\n",
        "for label in target_labels:\n",
        "  emotion_dataset = emotion_dataset.add_column(label, dummy_values)\n",
        "emotion_dataset = emotion_dataset.map(map_data_to_column)\n",
        "emotion_dataset = emotion_dataset.remove_columns(['label'])\n",
        "\n",
        "emotion_dataset_csv_path = 'emotion.csv'\n",
        "emotion_dataset.to_csv(emotion_dataset_csv_path, index=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCJk57X8H6rh"
      },
      "source": [
        "##Process and save go emotions dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QyUOulKqtCVU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "def reformat_go_emotions():\n",
        "  datasets_go_emotions = load_dataset(\"go_emotions\", \"raw\")\n",
        "  dataset_list = []\n",
        "  \n",
        "  for go_dataset in datasets_go_emotions:\n",
        "    go_dataset = datasets_go_emotions.get(go_dataset)\n",
        "    go_dataset = go_dataset.rename_column(\"joy\", \"happiness\")\n",
        "    qualifying_data_index = []\n",
        "    for go_index, go_data in enumerate(go_dataset):\n",
        "      sum_of_target_labels = 0\n",
        "      for label in target_labels:\n",
        "        sum_of_target_labels += go_data[label]\n",
        "      if sum_of_target_labels >= 1:\n",
        "        qualifying_data_index.append(go_index)\n",
        "    go_dataset = go_dataset.select(qualifying_data_index)\n",
        "    dataset_list.append(go_dataset)\n",
        "  filtered_dataset = concatenate_datasets(dataset_list)\n",
        "  \n",
        "  features_to_keep = target_labels + ['text']\n",
        "  for feature in filtered_dataset.features:\n",
        "    if feature not in features_to_keep:\n",
        "      filtered_dataset = filtered_dataset.remove_columns([feature])\n",
        "  return filtered_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr7Xrq_v7P6B"
      },
      "outputs": [],
      "source": [
        "go_emotions_dataset = reformat_go_emotions()\n",
        "go_emotions_dataset_csv_path = \"go_emotions.csv\"\n",
        "go_emotions_dataset.to_csv(go_emotions_dataset_csv_path, index=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hCxqy2JLMcP"
      },
      "source": [
        "#Preprocess Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N2ZqVeuLCFV"
      },
      "source": [
        "##Import datasets from fs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfu7dwsBLA84"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "go_emotions_dataset_csv_path = \"go_emotions.csv\"\n",
        "labelled_emotion_dataset_csv_path = \"emotion.csv\"\n",
        "\n",
        "go_emotions_dataset = load_dataset(\"csv\", data_files=go_emotions_dataset_csv_path)['train'].select(range(2000))\n",
        "emotion_dataset = load_dataset(\"csv\", data_files=labelled_emotion_dataset_csv_path)['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JpPJiiFMSev"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "def organize_labels_into_single_feature(example):\n",
        "  example['label'] = []\n",
        "  for label in target_labels:\n",
        "    example['label'].append(example[label])\n",
        "  return example\n",
        "\n",
        "dataset = concatenate_datasets([go_emotions_dataset, emotion_dataset])\n",
        "dataset = dataset.map(organize_labels_into_single_feature)\n",
        "dataset = dataset.remove_columns(target_labels)\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "dataset_text = []\n",
        "for text in dataset['text']:\n",
        "  dataset_text.append([text])\n",
        "\n",
        "x_train, y_train, x_val, y_val = iterative_train_test_split(np.array(dataset_text), np.array(dataset['label']), test_size=0.3)\n",
        "\n",
        "print(x_train[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhXdnItGxRir"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN2KrUKTxjNf"
      },
      "source": [
        "# Bert Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO8D3M3pB3Cj"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSh2hBeaximI"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "model = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model)\n",
        "sample = \"hello how are you\"\n",
        "bert_input = tokenizer(sample, return_tensors=\"pt\")\n",
        "print(bert_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGt3hgK4B6H1"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYSG86NfyEOM"
      },
      "outputs": [],
      "source": [
        "from torch import nn \n",
        "from transformers import BertModel, BertForSequenceClassification,TrainingArguments, Trainer\n",
        "labels = [\"happiness\", \"surprise\", \"disgust\", \"anger\", \"neutral\"]\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "classifier = BertForSequenceClassification.from_pretrained(model, num_labels=5, problem_type=\"multi_label_classification\",\n",
        "                                                               id2label=id2label,\n",
        "                                                               label2id=label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zscSDqRVB9U9"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24wYgd55yagR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset, ClassLabel, DatasetDict\n",
        "train = []\n",
        "for i in range(len(x_train)):\n",
        "  data, label = x_train[i][0], y_train[i]\n",
        "  dict_ = {\"text\":data}\n",
        "  for j in range(len(labels)):\n",
        "    dict_[labels[j]] = label[j]\n",
        "  train.append(dict_)\n",
        "\n",
        "eval = []\n",
        "for i in range(len(x_val)):\n",
        "  data, label = x_val[i][0], y_val[i]\n",
        "  dict_ = {\"text\":data}\n",
        "  for j in range(len(labels)):\n",
        "    dict_[labels[j]] = label[j]\n",
        "  eval.append(dict_)\n",
        "dataset_ = DatasetDict({\"train\":Dataset.from_list(train),\n",
        "                    \"eval\":Dataset.from_list(eval)})\n",
        "    \n",
        "\n",
        "print(dataset_[\"train\"][0])\n",
        "def preprocess_data(examples):\n",
        "  text = examples['text']\n",
        "  \n",
        "  bert_token = tokenizer(text, truncation=True, return_tensors=\"pt\", padding=\"max_length\", max_length=128)\n",
        "  labels_batch = {k:examples[k] for k in examples.keys() if k in labels}\n",
        "  labels_matrix = np.zeros((len(text), len(labels)))\n",
        "  for idx, label in enumerate(labels):\n",
        "    labels_matrix[:, idx] = labels_batch[label]\n",
        "  bert_token[\"labels\"] = labels_matrix.tolist()\n",
        "  return bert_token\n",
        "\n",
        "encode_set = dataset_.map(preprocess_data, batched=True, remove_columns=dataset_['train'].column_names)\n",
        "encode_set.set_format(\"torch\")\n",
        "train_set = encode_set[\"train\"]\n",
        "eval_set = encode_set[\"eval\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8WwEWSGabyX"
      },
      "outputs": [],
      "source": [
        "print(encode_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZYdABTqCCKk"
      },
      "source": [
        "## Fine-tune model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(id2label)\n",
        "print(label2id)\n",
        "print([id2label[idx] for idx, label in enumerate(train_set[0][\"labels\"]) if label == 1.0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHkTAZLizduS",
        "outputId": "a084b39d-2c6f-4848-96c4-95368e10dd75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'happiness', 1: 'surprise', 2: 'disgust', 3: 'anger', 4: 'neutral'}\n",
            "{'happiness': 0, 'surprise': 1, 'disgust': 2, 'anger': 3, 'neutral': 4}\n",
            "['anger']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "batch = 16\n",
        "wd = 1e-5\n",
        "lr =1e-5\n",
        "def compute_metrics(eval_pred):\n",
        "  logits, labels = eval_pred\n",
        "  prediction = torch.sigmoid(torch.tensor(logits)) > 0.5\n",
        "  match = prediction == torch.tensor(labels)\n",
        "  return {\"accuracy\": match.sum().item() / (logits.shape[0] * logits.shape[1])}\n",
        "arg =  TrainingArguments(output_dir=\"ece1786\", \n",
        "                         evaluation_strategy=\"epoch\",\n",
        "                          num_train_epochs=5,\n",
        "                          learning_rate=lr,\n",
        "                          weight_decay=wd,\n",
        "                          per_device_train_batch_size=batch,\n",
        "                          per_device_eval_batch_size=batch,\n",
        "                          logging_strategy=\"epoch\",\n",
        "                          save_strategy=\"epoch\"\n",
        "                          )\n",
        "\n",
        "trainer = Trainer(model=classifier,\n",
        "                  args=arg,\n",
        "                  train_dataset=train_set,\n",
        "                  eval_dataset=eval_set,\n",
        "                  compute_metrics=compute_metrics,\n",
        "                  tokenizer=tokenizer)\n",
        "result = trainer.train()"
      ],
      "metadata": {
        "id": "zuUluK-r5wvs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}